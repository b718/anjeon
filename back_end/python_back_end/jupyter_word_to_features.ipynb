{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from hashlib import sha1\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import sklearn \n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import (\n",
        "    GridSearchCV,\n",
        "    cross_val_score,\n",
        "    cross_validate,\n",
        "    train_test_split,\n",
        ")\n",
        "\n",
        "from stop_words import get_stop_words \n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline, make_pipeline                "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Cyberbullying Tweets csv file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                          tweet_text cyberbullying_type\n",
            "0  In other words #katandandre, your food was cra...  not_cyberbullying\n",
            "1  Why is #aussietv so white? #MKR #theblock #ImA...  not_cyberbullying\n",
            "2  @XochitlSuckkks a classy whore? Or more red ve...  not_cyberbullying\n",
            "3  @Jason_Gio meh. :P  thanks for the heads up, b...  not_cyberbullying\n",
            "4  @RudhoeEnglish This is an ISIS account pretend...  not_cyberbullying\n"
          ]
        }
      ],
      "source": [
        "tweets_df = pd.read_csv(\"controller/machine_learning/data/cyberbullying_tweets.csv\")\n",
        "print(tweets_df.head())\n",
        "stop_words_set = set(get_stop_words('en'))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "cyberbullying_type\n",
              "religion               7998\n",
              "age                    7992\n",
              "gender                 7973\n",
              "ethnicity              7961\n",
              "not_cyberbullying      7945\n",
              "other_cyberbullying    7823\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tweets_df[\"cyberbullying_type\"].value_counts()\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Modify Label if the value is 'not_cyberbullying' then 0, otherwise 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "cyberbullying_type\n",
              "1    39747\n",
              "0     7945\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tweets_df[\"cyberbullying_type\"] = tweets_df[\"cyberbullying_type\"].apply(lambda x: 0 if x == \"not_cyberbullying\" else 1)\n",
        "tweets_df[\"cyberbullying_type\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Each `tweet_text` Filtering (Also, Read **Further Notice**)\n",
        "<hr>\n",
        "\n",
        "`tweet_text` contains emoji or any other languages. To make model easily, we are going to stick to have only English.\n",
        "\n",
        "[**Further Notice**] As we know, if we remove the any emoji or any other languages, we might lose useful information. Then, we might need to change this process in the future.\n",
        "\n",
        "We also want to make all the text lowercase, remove stop words, and remove all links!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                          tweet_text  cyberbullying_type\n",
            "0            words katandandre food crapilicious mkr                   0\n",
            "1  aussietv white mkr theblock imacelebrityau tod...                   0\n",
            "2    xochitlsuckkks classy whore red velvet cupcakes                   0\n",
            "3  jasongio meh p thanks heads concerned another ...                   0\n",
            "4  rudhoeenglish isis account pretending kurdish ...                   0\n",
            "                                          tweet_text  cyberbullying_type\n",
            "0            words katandandre food crapilicious mkr                   0\n",
            "1  aussietv white mkr theblock imacelebrityau tod...                   0\n",
            "2    xochitlsuckkks classy whore red velvet cupcakes                   0\n",
            "3  jasongio meh p thanks heads concerned another ...                   0\n",
            "4  rudhoeenglish isis account pretending kurdish ...                   0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'kids love mohamad bin zayed city'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(tweets_df.head())\n",
        "tweets_df[\"tweet_text\"] = tweets_df[\"tweet_text\"].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
        "tweets_df[\"tweet_text\"] = tweets_df[\"tweet_text\"].apply(lambda x: re.sub(r'[^\\x00-\\x7F]+', '', x))\n",
        "tweets_df[\"tweet_text\"] = tweets_df[\"tweet_text\"].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]+', '', x))\n",
        "tweets_df[\"tweet_text\"] = tweets_df[\"tweet_text\"].apply(lambda x: x.lower())\n",
        "tweets_df[\"tweet_text\"] = tweets_df[\"tweet_text\"].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words_set]))\n",
        "print(tweets_df.head())\n",
        "\n",
        "#Check this below\n",
        "tweets_df[\"tweet_text\"][21]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = tweets_df[\"tweet_text\"]\n",
        "y = tweets_df[\"cyberbullying_type\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_vectorized = vectorizer.fit_transform(X_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.861478885273956"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 2. Create a vectorizer object with stop_words = \"english\"\n",
        "countvec = CountVectorizer(stop_words=\"english\")\n",
        "\n",
        "# 3. Create a logisticRegression object\n",
        "lr = LogisticRegression(max_iter=1000, random_state=123)\n",
        "# lr = LogisticRegression()\n",
        "\n",
        "# 4. Make a pipeline object\n",
        "pipe = make_pipeline(countvec, lr)\n",
        "\n",
        "pipe.fit(X_train, y_train)\n",
        "\n",
        "# 5. Store the mean values of cross-validation scores\n",
        "cv_score = cross_val_score(pipe, X_train, y_train).mean()\n",
        "cv_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SVM RBF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8593820027400823"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 2. Create a vectorizer object with stop_words = \"english\"\n",
        "countvec = CountVectorizer(stop_words=\"english\")\n",
        "\n",
        "# 3. Create a SVM RBF object\n",
        "svm = SVC(kernel=\"rbf\", C=10, gamma=0.1)\n",
        "\n",
        "# 4. Make a pipeline object\n",
        "pipe = make_pipeline(countvec, svm)\n",
        "\n",
        "# 5. Store the mean values of cross-validation scores\n",
        "cv_score = cross_val_score(pipe, X_train, y_train).mean()\n",
        "cv_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### RandomForest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "words katandandre food crapilicious mkr\n",
            "0.8560645769996855\n",
            "[[0.95916667 0.04083333]]\n"
          ]
        }
      ],
      "source": [
        "# 2. Create a vectorizer object with stop_words = \"english\"\n",
        "countvec = CountVectorizer(stop_words=\"english\")\n",
        "\n",
        "# 3. Create a RandomForest object\n",
        "rf = RandomForestClassifier(n_jobs=-1, random_state=123)\n",
        "\n",
        "# 4. Make a pipeline object\n",
        "pipe = make_pipeline(countvec, rf)\n",
        "\n",
        "# 5. Store the mean values of cross-validation scores\n",
        "cv_score = cross_val_score(pipe, X_train, y_train).mean()\n",
        "cv_score\n",
        "\n",
        "test = \"i love you\"\n",
        "\n",
        "pipe.fit(X_train, y_train)\n",
        "print(tweets_df[\"tweet_text\"][0])\n",
        "result = pipe.predict_proba([tweets_df[\"tweet_text\"][0]])\n",
        "print(result)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
